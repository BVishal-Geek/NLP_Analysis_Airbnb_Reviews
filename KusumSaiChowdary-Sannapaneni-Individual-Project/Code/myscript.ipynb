{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498196aa-0c3e-4665-b8c3-7443ecf54ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Phase1_SentimentalCode.py\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import emoji\n",
    "from transformers import pipeline, BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 1: Load Data\n",
    "csv_file_path = 'NYC_2021_airbnb_reviews_data1.csv'\n",
    "print(\"Loading data...\")\n",
    "data = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Step 2: Date Parsing\n",
    "print(\"Parsing dates...\")\n",
    "data['review_posted_date'] = pd.to_datetime(data['review_posted_date'], format='%B %Y', errors='coerce')\n",
    "data = data.dropna(subset=['review_posted_date'])  # Drop rows with invalid dates\n",
    "\n",
    "# Step 3: Handle Missing Values\n",
    "print(\"Handling missing values...\")\n",
    "if 'review' in data.columns:\n",
    "    num_missing_reviews = data['review'].isna().sum()\n",
    "    if num_missing_reviews > 0:\n",
    "        print(f\"Number of missing reviews: {num_missing_reviews}\")\n",
    "        data.dropna(subset=['review'], inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "else:\n",
    "    raise KeyError(\"The dataset does not contain a 'review' column.\")\n",
    "\n",
    "# Step 4: Detect Language\n",
    "print(\"Detecting language...\")\n",
    "from langdetect import detect\n",
    "tqdm.pandas(desc=\"Detecting language\")\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text.strip()) if len(text.strip()) > 3 else 'unknown'\n",
    "    except Exception:\n",
    "        return 'unknown'\n",
    "\n",
    "data['language'] = data['review'].progress_apply(detect_language)\n",
    "data = data[data['language'] == 'en']\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Step 5: Text Preprocessing\n",
    "print(\"Expanding contractions and cleaning text...\")\n",
    "import contractions\n",
    "\n",
    "def expand_contractions(text):\n",
    "    try:\n",
    "        return contractions.fix(text)\n",
    "    except Exception:\n",
    "        return text\n",
    "\n",
    "def clean_text(text):\n",
    "    try:\n",
    "        text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "        text = emoji.demojize(text)          # Convert emojis to text descriptions\n",
    "        text = re.sub(r'@\\w+|#\\w+', '', text)  # Remove mentions and hashtags\n",
    "        # Keep colons and underscores to preserve emoji descriptions\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s_:]', '', text)  # Remove special characters\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "        return text\n",
    "    except Exception:\n",
    "        return text\n",
    "\n",
    "data['review_expanded'] = data['review'].apply(expand_contractions)\n",
    "data['review_cleaned_text'] = data['review_expanded'].apply(clean_text)\n",
    "\n",
    "\n",
    "# Preprocess further for tokenization\n",
    "print(\"Further preprocessing for tokenization...\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english')) - {\n",
    "    'not', 'no', 'nor', 'never',   # Negations\n",
    "    'very', 'extremely', 'really', # Intensifiers\n",
    "    'i', 'we', 'my', 'you', 'your', 'our', 'us',   # Pronouns (lowercased)\n",
    "}\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    words = nltk.word_tokenize(text)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "data['cleaned_review'] = data['review_cleaned_text'].apply(preprocess_text)\n",
    "\n",
    "# Step 6: Initial Sentiment Analysis with Hugging Face Dataset\n",
    "print(\"Performing initial sentiment analysis efficiently...\")\n",
    "sentiment_pipeline = pipeline(\n",
    "    'sentiment-analysis',\n",
    "    model='distilbert-base-uncased-finetuned-sst-2-english',\n",
    "    device=0  # Set to -1 if no GPU available\n",
    ")\n",
    "\n",
    "# Create a Hugging Face Dataset from the data\n",
    "review_dataset = Dataset.from_pandas(data[['cleaned_review']])\n",
    "\n",
    "# Apply the sentiment pipeline directly to the dataset\n",
    "def analyze_sentiment(batch):\n",
    "    # Truncate reviews to a maximum of 512 characters\n",
    "    truncated_reviews = [review[:512] for review in batch['cleaned_review']]\n",
    "    batch['sentiment'] = sentiment_pipeline(truncated_reviews)\n",
    "    return batch\n",
    "\n",
    "# Map the function to the dataset\n",
    "review_dataset = review_dataset.map(analyze_sentiment, batched=True)\n",
    "\n",
    "# Extract sentiment results\n",
    "data['initial_sentiment'] = [result['label'] for result in review_dataset['sentiment']]\n",
    "data['sentiment_label'] = data['initial_sentiment'].map({'POSITIVE': 1, 'NEGATIVE': 0})\n",
    "\n",
    "# Step 7: Train-Test Split with Stratification\n",
    "print(\"Splitting data into training and validation sets...\")\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    data['cleaned_review'], data['sentiment_label'], test_size=0.2, random_state=42, stratify=data['sentiment_label']\n",
    ")\n",
    "\n",
    "# Step 8: Prepare Dataset for Hugging Face\n",
    "print(\"Preparing datasets...\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['cleaned_review'],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "train_data = Dataset.from_pandas(pd.DataFrame({'cleaned_review': train_texts, 'label': train_labels}))\n",
    "val_data = Dataset.from_pandas(pd.DataFrame({'cleaned_review': val_texts, 'label': val_labels}))\n",
    "\n",
    "train_data = train_data.map(tokenize_function, batched=True)\n",
    "val_data = val_data.map(tokenize_function, batched=True)\n",
    "\n",
    "train_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "val_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Step 9: Define Model and Trainer\n",
    "print(\"Initializing BERT model...\")\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    eval_strategy='epoch',  # Updated to the correct argument\n",
    "    save_strategy='epoch',\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Step 10: Train and Evaluate\n",
    "print(\"Training the model...\")\n",
    "trainer.train()\n",
    "print(\"Evaluating the model...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation results:\", eval_results)\n",
    "\n",
    "# Save Processed Data\n",
    "print(\"Saving processed data...\")\n",
    "data.to_csv(\"processed_reviews_SentimentLabels.csv\", index=False)\n",
    "print(\"Process completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a91393-0fa5-4f14-a105-7721470062bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment_plots.ipynb\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Ensure plots are displayed inline in Jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "# Set seaborn style for aesthetics\n",
    "sns.set(style='whitegrid', context='talk', palette='muted')\n",
    "\n",
    "# Step 1: Load Processed Data\n",
    "file_path = 'processed_reviews_SentimentLabels.csv'  # Replace with the correct file path\n",
    "print(\"Loading processed data...\")\n",
    "data = pd.read_csv(file_path)\n",
    "data.head()\n",
    "\n",
    "\n",
    "# Count the number of positive and negative sentiments\n",
    "sentiment_counts = data['sentiment_label'].value_counts().sort_index()\n",
    "\n",
    "# Map numerical labels to sentiment names\n",
    "sentiment_counts.index = sentiment_counts.index.map({0: 'Negative', 1: 'Positive'})\n",
    "\n",
    "# Plotting the bar graph\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette=['#FF6F61', '#6B5B95'])\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Sentiment Distribution of Airbnb Reviews', fontsize=18, fontweight='bold')\n",
    "plt.xlabel('Sentiment', fontsize=14)\n",
    "plt.ylabel('Number of Reviews', fontsize=14)\n",
    "\n",
    "# Annotate the bars with counts\n",
    "for i, count in enumerate(sentiment_counts.values):\n",
    "    plt.text(i, count + 50, f'{count}', ha='center', fontsize=12)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Ensure 'review_posted_date' is in datetime format\n",
    "data['review_posted_date'] = pd.to_datetime(data['review_posted_date'])\n",
    "\n",
    "# Extract year for grouping\n",
    "data['year'] = data['review_posted_date'].dt.to_period('Y')\n",
    "\n",
    "# Group by year and sentiment\n",
    "sentiment_over_year = data.groupby(['year', 'sentiment_label']).size().unstack(fill_value=0)\n",
    "sentiment_over_year.columns = sentiment_over_year.columns.map({0: 'Negative', 1: 'Positive'})\n",
    "\n",
    "# Plotting the time series by year\n",
    "plt.figure(figsize=(14, 7))\n",
    "ax = plt.gca()  # Get current axis\n",
    "\n",
    "# Color and style settings for yearly data\n",
    "colors = {'Positive': '#6B5B95', 'Negative': '#FF6F61'}\n",
    "marker_styles = {'Positive': 'o', 'Negative': 'X'}\n",
    "\n",
    "# Plot each column with custom settings\n",
    "for column in sentiment_over_year.columns:\n",
    "    sentiment_over_year[column].plot(\n",
    "        kind='line',\n",
    "        marker=marker_styles[column],\n",
    "        color=colors[column],\n",
    "        ax=ax,\n",
    "        linewidth=2,\n",
    "        markersize=8,\n",
    "        label=column\n",
    "    )\n",
    "\n",
    "# Adding titles and labels with enhanced formatting\n",
    "plt.title('Yearly Sentiment Trend for Airbnb Reviews', fontsize=20, fontweight='bold', color='#333333')\n",
    "plt.xlabel('Year', fontsize=16, color='#333333')\n",
    "plt.ylabel('Number of Reviews', fontsize=16, color='#333333')\n",
    "plt.xticks(rotation=0, fontsize=12, color='#666666')\n",
    "plt.yticks(fontsize=12, color='#666666')\n",
    "\n",
    "# Adding a grid\n",
    "plt.grid(True, linestyle='--', linewidth=0.5, color='grey', alpha=0.7)\n",
    "\n",
    "# Enhance legend\n",
    "plt.legend(title=\"Sentiment Type\", title_fontsize='13', fontsize='12', loc='upper left')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d706cd1-2094-424f-8545-beb3764dbcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ABSA_DataProcessing.py\n",
    "\n",
    "# Import standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import gensim\n",
    "import warnings\n",
    "import random\n",
    "import os  # To handle directory operations\n",
    "import json  # For loading manual cluster labels if using external file\n",
    "\n",
    "# Define a fixed seed for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "# Set seeds for random, numpy, and gensim\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Suppress warnings for clean output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import libraries for word embeddings and clustering\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Import NLTK modules\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Step 1: Loading Data\n",
    "print(\"\\nStep 1: Loading Data\")\n",
    "print(\"Loading the preprocessed data from 'processed_reviews_SentimentLabels.csv'...\")\n",
    "data = pd.read_csv('processed_reviews_SentimentLabels.csv')\n",
    "\n",
    "# Check for necessary columns\n",
    "required_columns = ['listing_id', 'review_posted_date', 'cleaned_review', 'sentiment_label']\n",
    "if all(column in data.columns for column in required_columns):\n",
    "    print(\"All required columns are present.\")\n",
    "else:\n",
    "    missing_cols = [col for col in required_columns if col not in data.columns]\n",
    "    raise ValueError(f\"Missing columns: {missing_cols}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nSample data:\")\n",
    "print(data.head())\n",
    "\n",
    "# Step 2: Tokenization and Lemmatization\n",
    "print(\"\\nStep 2: Tokenization and Lemmatization\")\n",
    "print(\"Initializing lemmatizer and tokenizing reviews...\")\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def tokenize_and_lemmatize(text):\n",
    "    try:\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n",
    "        return lemmatized_tokens\n",
    "    except Exception as e:\n",
    "        print(f\"Error in tokenization and lemmatization: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "data['tokens'] = data['cleaned_review'].apply(tokenize_and_lemmatize)\n",
    "\n",
    "# Display sample tokens\n",
    "print(\"\\nSample tokens after lemmatization:\")\n",
    "print(data[['listing_id', 'review_posted_date', 'cleaned_review', 'tokens']].head())\n",
    "\n",
    "# Step 3: Training the Word2Vec Model\n",
    "print(\"\\nStep 3: Training the Word2Vec Model\")\n",
    "print(\"Training Word2Vec model on tokenized reviews...\")\n",
    "sentences = data['tokens'].tolist()\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    workers=1,  # Set workers=1 for determinism\n",
    "    epochs=10,\n",
    "    seed=SEED  # Set the seed here\n",
    ")\n",
    "\n",
    "print(\"Word2Vec model training completed.\")\n",
    "print(f\"Vocabulary size: {len(w2v_model.wv)} words\")\n",
    "\n",
    "# Step 4: Extracting Nouns from Vocabulary\n",
    "print(\"\\nStep 4: Extracting Nouns from Vocabulary\")\n",
    "print(\"Performing POS tagging to extract nouns from the vocabulary...\")\n",
    "vocab = sorted(list(w2v_model.wv.index_to_key))  # Sort the vocabulary for consistency\n",
    "pos_tags = nltk.pos_tag(vocab)\n",
    "nouns = [word for word, pos in pos_tags if pos.startswith('NN')]\n",
    "\n",
    "print(f\"Total words in vocabulary: {len(vocab)}\")\n",
    "print(f\"Total nouns extracted: {len(nouns)}\")\n",
    "print(\"\\nSample nouns:\")\n",
    "print(nouns[:20])\n",
    "\n",
    "# Step 5: Obtaining Embeddings for Nouns\n",
    "print(\"\\nStep 5: Obtaining Embeddings for Nouns\")\n",
    "print(\"Retrieving embeddings for the extracted nouns...\")\n",
    "noun_embeddings = []\n",
    "nouns_filtered = []\n",
    "for noun in nouns:\n",
    "    if noun in w2v_model.wv:\n",
    "        noun_embeddings.append(w2v_model.wv[noun])\n",
    "        nouns_filtered.append(noun)\n",
    "\n",
    "noun_embeddings = np.array(noun_embeddings)\n",
    "nouns = sorted(nouns_filtered)  # Sort nouns to ensure consistent ordering\n",
    "\n",
    "print(f\"Total nouns with embeddings: {len(noun_embeddings)}\")\n",
    "\n",
    "# Step 6: Clustering Noun Embeddings\n",
    "print(\"\\nStep 6: Clustering Noun Embeddings\")\n",
    "print(\"Clustering noun embeddings using KMeans to identify aspects...\")\n",
    "num_clusters = 15\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=SEED, n_init=10)  # Set n_init explicitly\n",
    "nouns_clusters = pd.DataFrame({'noun': nouns})\n",
    "nouns_clusters['cluster'] = kmeans.fit_predict(noun_embeddings)\n",
    "\n",
    "print(\"\\nNumber of nouns in each cluster:\")\n",
    "print(nouns_clusters['cluster'].value_counts().sort_index())\n",
    "\n",
    "# Step 7: Assigning Aspect Names to Clusters\n",
    "print(\"\\nStep 7: Assigning Aspect Names to Clusters\")\n",
    "print(\"Printing nouns in each cluster for manual aspect mapping...\")\n",
    "\n",
    "# Create a directory to save cluster contents (optional)\n",
    "clusters_dir = 'clusters_output'\n",
    "os.makedirs(clusters_dir, exist_ok=True)\n",
    "\n",
    "# Iterate through each cluster and print/save its nouns\n",
    "for cluster_num in range(num_clusters):\n",
    "    cluster_nouns = nouns_clusters[nouns_clusters['cluster'] == cluster_num]['noun'].tolist()\n",
    "    print(f\"\\n--- Cluster {cluster_num} ---\")\n",
    "    print(cluster_nouns)\n",
    "\n",
    "    # Save to a text file for easier inspection\n",
    "    with open(os.path.join(clusters_dir, f'cluster_{cluster_num}.txt'), 'w') as f:\n",
    "        for noun in cluster_nouns:\n",
    "            f.write(f\"{noun}\\n\")\n",
    "\n",
    "print(\"\\nAll clusters have been printed and saved to the 'clusters_output' directory.\")\n",
    "print(\"Please review the clusters and manually assign aspect names accordingly.\")\n",
    "\n",
    "# Define the manual cluster labels based on your review\n",
    "manual_cluster_labels = {\n",
    "    0: 'Host Interaction and Personal Experience',\n",
    "    1: 'Nearby Amenities and Food Options',\n",
    "    2: 'Accommodation Facilities and Aesthetics',\n",
    "    3: 'Communication and Check-in Process',\n",
    "    4: 'People and Personal Interactions',\n",
    "    5: 'Amenities and Supplies',\n",
    "    6: 'Nearby Amenities and Food Options',\n",
    "    7: 'Accommodation Facilities and Aesthetics',\n",
    "    8: 'Issues and Complaints',\n",
    "    9: 'Nearby Amenities and Food Options',\n",
    "    10: 'Location and Transportation',\n",
    "    11: 'Value and Pricing',\n",
    "    12: 'Location and Transportation',\n",
    "    13: 'Transportation',\n",
    "    14: 'Accommodation Comfort and Issues'\n",
    "}\n",
    "\n",
    "# Assign aspects based on manual mapping\n",
    "nouns_clusters['aspect'] = nouns_clusters['cluster'].map(manual_cluster_labels)\n",
    "\n",
    "# Handle any missing aspects\n",
    "nouns_clusters['aspect'].fillna('Other', inplace=True)\n",
    "\n",
    "print(\"\\nSample nouns with their assigned aspects:\")\n",
    "print(nouns_clusters.head(20))\n",
    "\n",
    "# Step 8: Associating Aspects with Reviews\n",
    "print(\"\\nStep 8: Associating Aspects with Reviews\")\n",
    "print(\"Creating a mapping of nouns to aspects and assigning aspects to each review...\")\n",
    "\n",
    "# Create a dictionary mapping nouns to their aspects\n",
    "noun_to_aspect = pd.Series(nouns_clusters['aspect'].values, index=nouns_clusters['noun']).to_dict()\n",
    "\n",
    "\n",
    "def get_aspects_from_tokens(tokens):\n",
    "    aspects = set()\n",
    "    for token in tokens:\n",
    "        if token in noun_to_aspect:\n",
    "            aspects.add(noun_to_aspect[token])\n",
    "    return list(aspects)\n",
    "\n",
    "\n",
    "data['aspects'] = data['tokens'].apply(get_aspects_from_tokens)\n",
    "\n",
    "print(\"\\nSample reviews with their associated aspects:\")\n",
    "print(data[['listing_id', 'review_posted_date', 'cleaned_review', 'tokens', 'aspects']].head())\n",
    "\n",
    "# Step 9: Creating a DataFrame for Aspect-Level Sentiments\n",
    "print(\"\\nStep 9: Creating a DataFrame for Aspect-Level Sentiments\")\n",
    "print(\"Exploding aspects to create a row for each aspect mentioned in a review...\")\n",
    "\n",
    "# Include 'listing_id' and 'review_posted_date' in the exploded DataFrame\n",
    "data_exploded = data.explode('aspects')\n",
    "data_exploded = data_exploded.dropna(subset=['aspects'])\n",
    "\n",
    "# Select the desired columns, including 'listing_id' and 'review_posted_date'\n",
    "aspect_sentiments = data_exploded[['listing_id', 'review_posted_date', 'cleaned_review', 'aspects', 'sentiment_label']]\n",
    "\n",
    "print(\"\\nSample aspect-level sentiment data:\")\n",
    "print(aspect_sentiments.head())\n",
    "\n",
    "# Step 10: Saving the Aspect-Level Sentiment Data\n",
    "print(\"\\nStep 10: Saving the Aspect-Level Sentiment Data\")\n",
    "print(\"Saving the aspect-level sentiment data to 'AspectbasedSentimentAnalysis.csv'...\")\n",
    "aspect_sentiments.to_csv('AspectbasedSentimentAnalysis.csv', index=False)\n",
    "print(\"Data saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0655eb-3107-456d-a667-66eb3614d309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ABSA_plots.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'AspectbasedSentimentAnalysis.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "data.head()\n",
    "\n",
    "\n",
    "# Prepare data for the heatmap\n",
    "heatmap_data = data.groupby(['aspects', 'sentiment_label']).size().unstack(fill_value=0)\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt='d', cmap='Blues', cbar=True)\n",
    "plt.title('Heatmap of Sentiments Across Aspects')\n",
    "plt.xlabel('Sentiment Label (0: Negative, 1: Positive)')\n",
    "plt.ylabel('Aspects')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Filter the data for listing ID 1217318\n",
    "listing_data = data[data['listing_id'] == 1217318]\n",
    "\n",
    "# Aggregate the sentiment counts for each aspect\n",
    "radar_data = listing_data.groupby(['aspects', 'sentiment_label']).size().unstack(fill_value=0)\n",
    "\n",
    "# Preparing data for radar chart\n",
    "aspects = radar_data.index\n",
    "positive_counts = radar_data[1] if 1 in radar_data.columns else [0] * len(aspects)\n",
    "negative_counts = radar_data[0] if 0 in radar_data.columns else [0] * len(aspects)\n",
    "\n",
    "# Normalize the values to fit a radar chart (optional)\n",
    "max_count = max(positive_counts.max(), negative_counts.max())\n",
    "positive_counts_normalized = positive_counts / max_count\n",
    "negative_counts_normalized = negative_counts / max_count\n",
    "\n",
    "# Radar chart setup\n",
    "categories = list(aspects)\n",
    "categories += categories[:1]  # Close the loop for radar chart\n",
    "\n",
    "positive_values = list(positive_counts_normalized) + [positive_counts_normalized.iloc[0]]\n",
    "negative_values = list(negative_counts_normalized) + [negative_counts_normalized.iloc[0]]\n",
    "\n",
    "# Angle calculation\n",
    "angles = [n / float(len(categories)) * 2 * 3.14159 for n in range(len(categories))]\n",
    "\n",
    "# Plotting radar chart\n",
    "plt.figure(figsize=(8, 8))\n",
    "ax = plt.subplot(111, polar=True)\n",
    "\n",
    "# Draw one axe per aspect + add labels\n",
    "ax.set_theta_offset(3.14159 / 2)\n",
    "ax.set_theta_direction(-1)\n",
    "plt.xticks(angles[:-1], aspects, color='grey', size=10)\n",
    "\n",
    "# Plot data\n",
    "ax.plot(angles, positive_values, linewidth=1, linestyle='solid', label='Positive Sentiment')\n",
    "ax.fill(angles, positive_values, 'b', alpha=0.1)\n",
    "\n",
    "ax.plot(angles, negative_values, linewidth=1, linestyle='solid', label='Negative Sentiment')\n",
    "ax.fill(angles, negative_values, 'r', alpha=0.1)\n",
    "\n",
    "# Add legend and title\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.4, 1.3))\n",
    "plt.title('Sentiment Distribution for Listing ID 1217318 Across Aspects', size=15, color='darkblue')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
